{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d1041519dca9ccef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:08:41.892049300Z",
     "start_time": "2024-09-22T19:42:07.414143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Load datasets\n",
    "train_data_path = 'data/train.csv'\n",
    "test_data_path = 'data/test.csv'\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Fill missing values \n",
    "train_data.fillna(train_data.mean(), inplace=True)\n",
    "test_data.fillna(train_data.mean(), inplace=True)\n",
    "\n",
    "# Encode categorical columns \n",
    "if 'Gender' in train_data.columns:\n",
    "    le = LabelEncoder()\n",
    "    train_data['Gender'] = le.fit_transform(train_data['Gender'])\n",
    "    test_data['Gender'] = le.transform(test_data['Gender'])\n",
    "\n",
    "# Feature engineering\n",
    "if 'BMI' in train_data.columns and 'Age' in train_data.columns:\n",
    "    train_data['BMI_Age'] = train_data['BMI'] / train_data['Age']\n",
    "    test_data['BMI_Age'] = test_data['BMI'] / test_data['Age']\n",
    "\n",
    "# Interaction features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interaction = poly.fit_transform(train_data.drop(columns=['Target', 'Id']))\n",
    "X_test_interaction = poly.transform(test_data.drop(columns=['Id']))\n",
    "\n",
    "# Combine original and interaction features\n",
    "X = np.hstack((train_data.drop(columns=['Target', 'Id']), X_interaction))\n",
    "y = train_data['Target']\n",
    "X_test = np.hstack((test_data.drop(columns=['Id']), X_test_interaction))\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle imbalanced data\n",
    "smote = SMote(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "param_dist = {\n",
    "    'max_depth': range(3, 7),\n",
    "    'n_estimators': range(100, 300, 100),\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'gamma': [0, 0.1],\n",
    "    'reg_lambda': [0.1, 1.0],\n",
    "    'reg_alpha': [0.1, 1.0],\n",
    "    'min_child_weight': range(1, 4)\n",
    "}\n",
    "\n",
    "# Measure the time taken for a single model fit \n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "single_fit_time = end_time - start_time\n",
    "print(f\"Time for a single fit: {single_fit_time:.2f} seconds\")\n",
    "\n",
    "# Estimate total time for RandomizedSearchCV\n",
    "total_fits = 150  # Based on 50 candidates and 3-fold CV\n",
    "number_of_jobs = 8  # Assume that n_jobs in RandomizedSearchCV is set to 8\n",
    "\n",
    "estimated_total_time = (single_fit_time * total_fits) / number_of_jobs\n",
    "print(f\"Estimated total time for RandomizedSearchCV: {estimated_total_time:.2f} seconds or {estimated_total_time/60:.2f} minutes\")\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=3,\n",
    "    verbose=3,    # Increased verbosity\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_random.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken for RandomizedSearchCV: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Save the search results\n",
    "joblib.dump(xgb_random, 'xgb_random_search.pkl')\n",
    "\n",
    "best_params = xgb_random.best_params_\n",
    "print(f\"Best parameters found by RandomizedSearchCV: {best_params}\")\n",
    "\n",
    "xgb_best_model = xgb_random.best_estimator_\n",
    "xgb_best_model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = xgb_best_model.predict(X_val)\n",
    "balanced_acc_score = balanced_accuracy_score(y_val, y_val_pred)\n",
    "print(f\"XGBoost Balanced Accuracy Score on Validation Set: {balanced_acc_score}\")\n",
    "\n",
    "# If satisfied with XGBoost, you can then proceed with the other models\n",
    "# Ensemble voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb_best_model,\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('rf', models['RandomForest']),\n",
    "    ('xgb', models['XGBoost']),\n",
    "    ('gbc', models['GradientBoosting'])\n",
    "], voting='soft')\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# Final predictions on test data\n",
    "test_predictions = voting_model.predict(X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'Id': test_data['Id'], 'Target': test_predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created successfully!\")\n"
   ],
   "id": "f37bb0b97daae483",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 77\u001B[0m\n\u001B[0;32m     55\u001B[0m param_dist \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_depth\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m10\u001B[39m),\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_estimators\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m1000\u001B[39m, \u001B[38;5;241m100\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin_child_weight\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m     65\u001B[0m }\n\u001B[0;32m     67\u001B[0m xgb_random \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(\n\u001B[0;32m     68\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mXGBClassifier(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m),\n\u001B[0;32m     69\u001B[0m     param_distributions\u001B[38;5;241m=\u001B[39mparam_dist,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     75\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     76\u001B[0m )\n\u001B[1;32m---> 77\u001B[0m xgb_random\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[0;32m     78\u001B[0m best_params \u001B[38;5;241m=\u001B[39m xgb_random\u001B[38;5;241m.\u001B[39mbest_params_\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest parameters found by RandomizedSearchCV: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_params\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1013\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1014\u001B[0m     )\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1018\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[0;32m   1020\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1957\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1958\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1959\u001B[0m     evaluate_candidates(\n\u001B[0;32m   1960\u001B[0m         ParameterSampler(\n\u001B[0;32m   1961\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_distributions, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_iter, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state\n\u001B[0;32m   1962\u001B[0m         )\n\u001B[0;32m   1963\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    957\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    958\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    959\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    960\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    961\u001B[0m         )\n\u001B[0;32m    962\u001B[0m     )\n\u001B[1;32m--> 964\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    965\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    966\u001B[0m         clone(base_estimator),\n\u001B[0;32m    967\u001B[0m         X,\n\u001B[0;32m    968\u001B[0m         y,\n\u001B[0;32m    969\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[0;32m    970\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[0;32m    971\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[0;32m    972\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[0;32m    973\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[0;32m    974\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[0;32m    975\u001B[0m     )\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params),\n\u001B[0;32m    978\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)),\n\u001B[0;32m    979\u001B[0m     )\n\u001B[0;32m    980\u001B[0m )\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    983\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    984\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    985\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    987\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     73\u001B[0m )\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1762\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
